{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Simple RNN application on sample data\n",
        "\n"
      ],
      "metadata": {
        "id": "AsmWvZSyg_xS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout, TimeDistributed\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Sample Data\n",
        "sentences = [\n",
        "    \"Barack Obama was born in Hawaii\",\n",
        "    \"Google is based in Mountain View\"\n",
        "]\n",
        "\n",
        "labels = [\n",
        "    [\"PERSON\", \"PERSON\", \"0\", \"0\", \"0\", \"LOCATION\"],\n",
        "    [\"ORGANIZATION\", \"0\", \"0\", \"0\", \"LOCATION\", \"LOCATION\"]\n",
        "]\n",
        "\n",
        "# Preprocessing\n",
        "# Adding oov_token so unknown words don't disappear\n",
        "tokenizer = Tokenizer(lower=True, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "X = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "# Padding\n",
        "X = pad_sequences(X, padding='post')\n",
        "\n",
        "# Encode the labels\n",
        "label_encoder = LabelEncoder()\n",
        "# We need to make sure '0', 'PERSON', 'LOCATION', 'ORGANIZATION' are in the encoder\n",
        "label_encoder.fit([\"0\", \"PERSON\", \"LOCATION\", \"ORGANIZATION\"])\n",
        "\n",
        "y = [label_encoder.transform(label) for label in labels]\n",
        "y = pad_sequences(y, padding=\"post\", maxlen=X.shape[1])\n",
        "y = np.expand_dims(y, -1)\n",
        "\n",
        "# Build the RNN Model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=50, input_length=X.shape[1]))\n",
        "model.add(SimpleRNN(units=50, return_sequences=True))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(TimeDistributed(Dense(len(label_encoder.classes_), activation='softmax')))\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(np.array(X), np.array(y), epochs=100, batch_size=2, verbose=0)\n",
        "print(\"Training complete\")\n",
        "\n",
        "# Test the Model\n",
        "test_sentence = [\"Barack Obama went to Hawaii\"]\n",
        "\n",
        "# Because we used oov_token, 'went' and 'to' will be kept as <OOV> instead of deleted\n",
        "test_sequence = tokenizer.texts_to_sequences(test_sentence)\n",
        "test_sequence = pad_sequences(test_sequence, padding='post', maxlen=X.shape[1])\n",
        "\n",
        "predictions = model.predict(test_sequence)\n",
        "decoded_predictions = label_encoder.inverse_transform(np.argmax(predictions, axis=-1)[0])\n",
        "\n",
        "print(\"\\nPrediction Results:\")\n",
        "# We limit the loop to the length of the actual sentence to avoid printing padding labels\n",
        "words = test_sentence[0].split()\n",
        "for i in range(len(words)):\n",
        "    print(f\"Word: {words[i]:<10} Predicted Label: {decoded_predictions[i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3Zn3ZUQDnIB",
        "outputId": "386711ba-41ed-4d2a-a26e-4e66a69c4dcb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training complete\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 757ms/step\n",
            "\n",
            "Prediction Results:\n",
            "Word: Barack     Predicted Label: PERSON\n",
            "Word: Obama      Predicted Label: PERSON\n",
            "Word: went       Predicted Label: 0\n",
            "Word: to         Predicted Label: 0\n",
            "Word: Hawaii     Predicted Label: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using NERdataset.csv from kaggle & changing the units to 100, epocs to 50 & learning rates to 0.01 for better results"
      ],
      "metadata": {
        "id": "YW0K95eShFVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast  # Library to handle stringified lists (e.g., \"['O', 'B-geo']\")\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout, TimeDistributed\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the Dataset\n",
        "try:\n",
        "    data = pd.read_csv('/content/ner.csv', encoding='unicode_escape')\n",
        "except FileNotFoundError:\n",
        "    print(\"ERROR: File not found. Please upload 'ner.csv' to the /content/ folder.\")\n",
        "    raise\n",
        "\n",
        "# The 'Tag' column is stored as a text string like \"['O', 'B-geo']\".\n",
        "# We use ast.literal_eval to convert it back into a real Python list.\n",
        "print(\"Processing tags\")\n",
        "data['Tag'] = data['Tag'].apply(ast.literal_eval)\n",
        "\n",
        "# We use the first 5000 sentences to ensure it runs quickly in Colab\n",
        "# (The full dataset might take too long for a simple tutorial test)\n",
        "sampled_data = data[:5000]\n",
        "\n",
        "sentences = sampled_data['Sentence'].astype(str).tolist()\n",
        "labels = sampled_data['Tag'].tolist()\n",
        "\n",
        "print(f\"Loaded {len(sentences)} sentences.\")\n",
        "# Preprocessing\n",
        "\n",
        "# A. Tokenize Sentences\n",
        "tokenizer = Tokenizer(lower=True, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "X = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "# Padding X (Input)\n",
        "# We set a max length based on the longest sentence in our sample\n",
        "max_len = max([len(x) for x in X])\n",
        "X = pad_sequences(X, padding='post', maxlen=max_len)\n",
        "\n",
        "# B. Encode Labels\n",
        "# Unlike the simple example, this dataset has many tags (B-geo, I-tim, etc.)\n",
        "# We must find ALL unique tags in our dataset dynamically\n",
        "label_encoder = LabelEncoder()\n",
        "all_tags = [tag for sublist in labels for tag in sublist] # Flatten the list\n",
        "unique_tags = list(set(all_tags))\n",
        "label_encoder.fit(unique_tags)\n",
        "\n",
        "print(f\"Detected {len(unique_tags)} unique tags: {unique_tags[:10]}\")\n",
        "\n",
        "# Transform text labels to numbers\n",
        "y = [label_encoder.transform(seq) for seq in labels]\n",
        "\n",
        "# Padding y (Labels)\n",
        "# Labels must be padded to the exact same length as X\n",
        "y = pad_sequences(y, padding=\"post\", maxlen=max_len, value=label_encoder.transform([\"O\"])[0])\n",
        "\n",
        "# Reshape y for the model (samples, time_steps, 1)\n",
        "y = np.expand_dims(y, -1)\n",
        "\n",
        "# Split into Training and Validation sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build the RNN Model\n",
        "print(\"Building model\")\n",
        "model = Sequential()\n",
        "\n",
        "# Input dim = vocab size + 1 (for padding)\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=50, input_length=max_len))\n",
        "\n",
        "# SimpleRNN Layer\n",
        "model.add(SimpleRNN(units=50, return_sequences=True))\n",
        "\n",
        "# Dropout to prevent overfitting\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "# Output Layer\n",
        "# Units = number of unique tags\n",
        "model.add(TimeDistributed(Dense(len(label_encoder.classes_), activation='softmax')))\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the Model\n",
        "print(\"Training model\")\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32)\n",
        "print(\"Training complete\")\n",
        "\n",
        "# Test with a New Sentence\n",
        "test_sentences = [\n",
        "    \"Barack Obama went to nazeer ka ashiyana\",\n",
        "    \"London is based in Paris\"\n",
        "]\n",
        "\n",
        "print(\"\\n Testing Results\")\n",
        "for sentence in test_sentences:\n",
        "    # Tokenize and Pad\n",
        "    seq = tokenizer.texts_to_sequences([sentence])\n",
        "    seq_padded = pad_sequences(seq, padding='post', maxlen=max_len)\n",
        "\n",
        "    # Predict\n",
        "    prediction = model.predict(seq_padded)\n",
        "\n",
        "    # Decode (Convert numbers back to Tag names)\n",
        "    # argmax finds the highest probability class\n",
        "    pred_indices = np.argmax(prediction, axis=-1)[0]\n",
        "    decoded_labels = label_encoder.inverse_transform(pred_indices)\n",
        "\n",
        "    print(f\"\\nSentence: {sentence}\")\n",
        "    print(f\"{'Word':<15} {'Predicted Label'}\")\n",
        "\n",
        "    words = sentence.split()\n",
        "    # We zip words with decoded_labels\n",
        "    # the prediction includes padding 'O's at the end, so we stop when words run out\n",
        "    for i, word in enumerate(words):\n",
        "        print(f\"{word:<15} {decoded_labels[i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDD67s2JK-dA",
        "outputId": "aa2656df-e647-4b2a-db78-31d5742d668e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing tags\n",
            "Loaded 5000 sentences.\n",
            "Detected 17 unique tags: ['O', 'I-tim', 'I-eve', 'B-tim', 'B-art', 'B-per', 'I-gpe', 'I-nat', 'B-geo', 'B-eve']\n",
            "Building model\n",
            "Training model\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 104ms/step - accuracy: 0.8828 - loss: 0.7570 - val_accuracy: 0.9479 - val_loss: 0.2669\n",
            "Epoch 2/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9485 - loss: 0.2546 - val_accuracy: 0.9508 - val_loss: 0.2289\n",
            "Epoch 3/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9516 - loss: 0.2145 - val_accuracy: 0.9533 - val_loss: 0.2084\n",
            "Epoch 4/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9545 - loss: 0.1926 - val_accuracy: 0.9544 - val_loss: 0.1940\n",
            "Epoch 5/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9571 - loss: 0.1739 - val_accuracy: 0.9560 - val_loss: 0.1845\n",
            "Epoch 6/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9616 - loss: 0.1517 - val_accuracy: 0.9554 - val_loss: 0.1794\n",
            "Epoch 7/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9667 - loss: 0.1297 - val_accuracy: 0.9564 - val_loss: 0.1764\n",
            "Epoch 8/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9686 - loss: 0.1167 - val_accuracy: 0.9557 - val_loss: 0.1784\n",
            "Epoch 9/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9720 - loss: 0.1032 - val_accuracy: 0.9554 - val_loss: 0.1830\n",
            "Epoch 10/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9750 - loss: 0.0900 - val_accuracy: 0.9530 - val_loss: 0.1913\n",
            "Epoch 11/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9768 - loss: 0.0830 - val_accuracy: 0.9523 - val_loss: 0.1994\n",
            "Epoch 12/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9799 - loss: 0.0724 - val_accuracy: 0.9507 - val_loss: 0.2103\n",
            "Epoch 13/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9829 - loss: 0.0615 - val_accuracy: 0.9508 - val_loss: 0.2210\n",
            "Epoch 14/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9847 - loss: 0.0545 - val_accuracy: 0.9483 - val_loss: 0.2356\n",
            "Epoch 15/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9869 - loss: 0.0488 - val_accuracy: 0.9495 - val_loss: 0.2425\n",
            "Epoch 16/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9880 - loss: 0.0432 - val_accuracy: 0.9464 - val_loss: 0.2595\n",
            "Epoch 17/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9899 - loss: 0.0375 - val_accuracy: 0.9459 - val_loss: 0.2712\n",
            "Epoch 18/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9911 - loss: 0.0338 - val_accuracy: 0.9470 - val_loss: 0.2788\n",
            "Epoch 19/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9914 - loss: 0.0322 - val_accuracy: 0.9444 - val_loss: 0.2943\n",
            "Epoch 20/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9917 - loss: 0.0304 - val_accuracy: 0.9477 - val_loss: 0.2961\n",
            "Epoch 21/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9927 - loss: 0.0265 - val_accuracy: 0.9459 - val_loss: 0.3052\n",
            "Epoch 22/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9933 - loss: 0.0241 - val_accuracy: 0.9442 - val_loss: 0.3214\n",
            "Epoch 23/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9936 - loss: 0.0239 - val_accuracy: 0.9448 - val_loss: 0.3254\n",
            "Epoch 24/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9936 - loss: 0.0226 - val_accuracy: 0.9445 - val_loss: 0.3343\n",
            "Epoch 25/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9942 - loss: 0.0212 - val_accuracy: 0.9451 - val_loss: 0.3369\n",
            "Epoch 26/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9948 - loss: 0.0194 - val_accuracy: 0.9442 - val_loss: 0.3462\n",
            "Epoch 27/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9950 - loss: 0.0185 - val_accuracy: 0.9442 - val_loss: 0.3534\n",
            "Epoch 28/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9952 - loss: 0.0176 - val_accuracy: 0.9433 - val_loss: 0.3618\n",
            "Epoch 29/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9954 - loss: 0.0166 - val_accuracy: 0.9418 - val_loss: 0.3742\n",
            "Epoch 30/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9955 - loss: 0.0156 - val_accuracy: 0.9415 - val_loss: 0.3780\n",
            "Epoch 31/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9957 - loss: 0.0154 - val_accuracy: 0.9417 - val_loss: 0.3868\n",
            "Epoch 32/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9961 - loss: 0.0138 - val_accuracy: 0.9433 - val_loss: 0.3870\n",
            "Epoch 33/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9958 - loss: 0.0140 - val_accuracy: 0.9473 - val_loss: 0.3767\n",
            "Epoch 34/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9959 - loss: 0.0143 - val_accuracy: 0.9425 - val_loss: 0.3969\n",
            "Epoch 35/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9959 - loss: 0.0145 - val_accuracy: 0.9405 - val_loss: 0.4063\n",
            "Epoch 36/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9963 - loss: 0.0131 - val_accuracy: 0.9431 - val_loss: 0.4016\n",
            "Epoch 37/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9967 - loss: 0.0116 - val_accuracy: 0.9445 - val_loss: 0.3991\n",
            "Epoch 38/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9966 - loss: 0.0119 - val_accuracy: 0.9443 - val_loss: 0.4036\n",
            "Epoch 39/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9969 - loss: 0.0109 - val_accuracy: 0.9411 - val_loss: 0.4212\n",
            "Epoch 40/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9971 - loss: 0.0103 - val_accuracy: 0.9401 - val_loss: 0.4281\n",
            "Epoch 41/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9969 - loss: 0.0110 - val_accuracy: 0.9408 - val_loss: 0.4321\n",
            "Epoch 42/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9968 - loss: 0.0111 - val_accuracy: 0.9424 - val_loss: 0.4284\n",
            "Epoch 43/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9971 - loss: 0.0099 - val_accuracy: 0.9431 - val_loss: 0.4316\n",
            "Epoch 44/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9966 - loss: 0.0114 - val_accuracy: 0.9411 - val_loss: 0.4384\n",
            "Epoch 45/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9972 - loss: 0.0099 - val_accuracy: 0.9438 - val_loss: 0.4320\n",
            "Epoch 46/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9971 - loss: 0.0095 - val_accuracy: 0.9418 - val_loss: 0.4424\n",
            "Epoch 47/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9953 - loss: 0.0171 - val_accuracy: 0.9429 - val_loss: 0.4247\n",
            "Epoch 48/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9957 - loss: 0.0148 - val_accuracy: 0.9429 - val_loss: 0.4260\n",
            "Epoch 49/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9966 - loss: 0.0113 - val_accuracy: 0.9456 - val_loss: 0.4274\n",
            "Epoch 50/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9971 - loss: 0.0094 - val_accuracy: 0.9423 - val_loss: 0.4392\n",
            "Training complete\n",
            "\n",
            " Testing Results\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7s/step\n",
            "\n",
            "Sentence: Barack Obama went to nazeer ka ashiyana\n",
            "Word            Predicted Label\n",
            "Barack          B-gpe\n",
            "Obama           O\n",
            "went            O\n",
            "to              O\n",
            "nazeer          B-tim\n",
            "ka              O\n",
            "ashiyana        O\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\n",
            "Sentence: London is based in Paris\n",
            "Word            Predicted Label\n",
            "London          B-geo\n",
            "is              O\n",
            "based           O\n",
            "in              O\n",
            "Paris           O\n"
          ]
        }
      ]
    }
  ]
}